# 支持向量机

[toc]

# 基本思路

> 是对(B站视频)[https://b23.tv/BV1Hs411w7ci/p1]的总结
>
> 参考 统计学习方法 李航

SVM的基本想法是求解能够正确划分训练数据集并且**几何间隔**最大的分离超平面.

SVM有三宝: 间隔/对偶/核技巧

# SVM定义

定义特征空间上的训练数据集:
$$
T=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
$$
其中$x_i\in \mathbb{R}^n$, $y\in\{+1, -1\}$. 特征向量类标记一一对应. 学习的目标是在特征空间一个分离超平面, 能够将实例分到不同的类, 分离超平面对应于方程 $w\cdot x + b = 0$, 通过法向量$w$ 和截距$b$ 决定, 可以用 $(w, b)$ 表示.

由于线性可分支持向量机间隔最大化求出分离超平面, 它的超平面是唯一的.

其分类决策函数为:
$$
f(x) = \text{sign}(w\cdot x + b)
$$


## 函数间隔和几何间隔

**函数间隔** 定义超平面$(w, b)$ 关于样本点 $(x_i, y_i)$ 的函数间隔为:
$$
\hat{\gamma_i} = y_i(w\cdot x_i + b)
$$
因为超平面上的点满足 $w\cdot x + b = 0$, 那么 $|w\cdot x_i + b|$可以相对地表示点$x_i$ 距离超平面的远近, 又因为对于二分类问题 $w\cdot x + b > 0$ 被判定为1, 反之-1, 所以可以通过 $y_i(w\cdot x_i + b)$ 来定义样本点与超平面之间的函数间隔.

定义超平面$(w, b)$ 关于训练数据集的函数间隔为所有样本点到超平面间隔的最小值:
$$
\hat{\gamma} = \min_{i = 1,...,N} \hat{\gamma_i}
$$
但这样定义存在的问题是, 只要成比例改变$w, b$ 虽然超平面没有变化, 但是函数间隔却也成比例改变了, 因此需要对超平面的法向量 $w$ 增加缺数, 例如规范化, 将函数间隔转化为几何间隔
$$
\gamma_i = y_i\left(\frac{w}{\|w\|}\cdot x_i + \frac{b}{\|w\|}\right)
$$

## 构造最大间隔分类器

需要求解几何间隔最大的分类超平面, 因此有:
$$
\begin{align}
& \max_{w, b} \gamma \\
& s.t.\ y_i(w\cdot x_i + b) > 0
\end{align}
$$
约束条件可以转化为:
$$
\exists \gamma>0, \min_{i=1,..,N} \frac{1}{\|w\|}y_i(w\cdot x_i + b) = \gamma
$$
那么问题可以转化为:
$$
\begin{align}
& \max_{w, b} \frac{\hat{\gamma}}{\|w\|}\\
& s.t.\ y_i(w\cdot x_i + b)\ge \hat{\gamma}
\end{align}
$$
注意函数间隔不影响最优化问题的解, 定义函数间隔为1, 然后把最大化函数转化为最小化函数的倒数, 并对其加平方(也不影响结果)
$$
\begin{align}
& \min_{w, b} \frac{1}{2} \|w\|^2\\
& s.t.\ y_i(w\cdot x_i + b) - 1\ge 0
\end{align}
$$


**什么是凸二次规划(convex quadratic programming)问题** 

凸优化问题值约束最优化问题:
$$
\begin{align}
& \min_{w} f(w) \\ 
& s.t.\ \begin{cases}g_i(w)\leq 0& i= 1, 2,...,k \\h_i(w)=0& i=1,2,...,l \end{cases}
\end{align}
$$
其中, 目标函数$g(w)$ 和约束函数 $g_i(w)$ 都是 $R^n$上的连续可微的凸函数, 约束函数 $h_i(w)$ 是$R^n$上的仿射函数, 即 $h(x) = a\cdot x + b$

当目标函数是二次函数且约束函数$g_i$是仿射函数时, 上述凸优化问题成为凸二次规划问题


## 线性可分支持向量机学习算法(最大间隔法)

假设最优解为 $w^*, b^*$

**最大间隔分离超平面的存在唯一性** 若训练数据集是线性可分, 则可将训练数据集中的样本单完全正确分开的最大间隔分离超平面存在且唯一. 证明看统计学习方法, 存在性和唯一性证明, 反证法.



## 支持向量和间隔边界

**support vector**在线性可分的情况下, 训练数据的样本点中与分离超平面距离最近的样本点的实例称为支持向量, 支持向量即约束公式等号成立的点
$$
y_i(w\cdot x_i + b) - 1 =0
$$
因此可以得到两个超平面, 对于正样本
$$
H_1: w\cdot x + b = 1
$$
对于负样本
$$
H_2: w\cdot x + b = -1
$$
在这两个超平面上的点为支持向量, $H_1, H_2$ 平行, 且中间没有样本点, 定义他们中间的距离为间隔(margin), 间隔依赖于分离超平面的法向量$w$, 等于 $2/\|w\|$

决定分离超平面的只有支持向量起作用, 支持向量一般很少.

>参考 直线的法向量表示 https://zhuanlan.zhihu.com/p/73397884

对于直线
$$
Ax+by+Cz = D
$$
其法向量为 $(A, B, C)$

## 对偶算法

### 拉格朗日对偶性(Lagrange Duality)

用拉格朗日对偶性, 可以原始问题转化为对偶问题, 并通过解对偶问题得到原始问题的解, 该方法可以应用在最大熵模型和支持向量机上.

**原始问题**

定义$f(x), c_i(x), h_j(x)$是$R^n$上的连续可微函数, 最优化问题
$$
\begin{align}
&\min_{x\in R^n} f(x)\\
&s.t. \begin{cases}
c_i(x) \leq 0 & i = 1, 2,...,k\\
h_j(x)=0& j = 1,2,...,l
\end{cases}
\end{align}
$$
引入广义拉格朗日函数(generalized Lagrange function)
$$
L(x,\alpha,\beta)=f(x)+ \sum_{i = 1}^k\alpha_i c_i(x)+\sum_{j=1}^l \beta_j h_j (x)
$$
$\alpha,\beta$ 为拉格朗日乘子, 且 $\alpha_i\ge 0$, 定义关于$x$的函数
$$
\theta_p(x)=\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha, \beta)
$$
下标$p$表示原始问题, 如果$x$满足原始问题的约束条件, 则极小化$\theta_p$与原始问题等价
$$
\min_x\theta_p(x) = \min_x\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)
$$
原始优化问题被表示为**广义拉格朗日函数的极小极大问题**, 定义原始问题的最优值为
$$
p^* =\min_x \theta_p(x)
$$
**对偶问题**

类似的:
$$
\theta_d(\alpha, \beta) = \min_x L(x,\alpha,\beta)
$$
则**广义拉格朗日函数的极大极小问题**被定义为
$$
\max_{\alpha,\beta:\alpha_i\ge0} \theta_d(\alpha,\beta) = \max_{\alpha,\beta:\alpha_i\ge0}\min_{x} L(x,\alpha,\beta)
$$
该问题可以表示为约束优化问题
$$
\begin{align}
&\max_{\alpha,\beta}\theta_d(\alpha,\beta)=\max_{\alpha,\beta}\min_xL(x,\alpha,\beta)\\
&s.t.\ \alpha_i\ge0 \quad i=1,2,...,k
\end{align}
$$
为原始问题的对偶问题, 定义其最优值为
$$
d^*=\max_{\alpha,\beta:\alpha_i\ge0} \theta_d(\alpha,\beta)
$$
为对偶问题的值

**原始问题和对偶问题的关系**

我理解对偶问题就是原始问题的下界, 因此极小化原始问题, 可以转化为极大化对偶问题.

统计学习方法给出了几条定理:

定理1: 若原始问题和对偶问题都有最优值, 则
$$
d^* \le p^*
$$
定理2: 若$x^*,\alpha^*,\beta^*$分别是原始问题和对偶问题的可行解, 并且$p^*=d^*$,则他们也是最优解

定理3: 如果$f(x), c_i(x)$为凸函数, $h_j(x)$是仿射函数, 并且假设不等式$c_i(x)$是严格可行, 即存在$x$, 使$c_i(x)<0$, 则原问题对偶问题最优解相同

>凸函数: 
>$$
>f\left(\frac{x_1 + x_2}{2}\right)\le\frac{f(x_1)+f(x_2)}{2}
>$$

### KKT条件

定理3 **KKT条件**

对于原始问题和对偶问题, 假设函数 $f(x), c_i(x)$是凸函数, $h_j(x)$是仿射函数, 且不等式约束$c_i(x)$是严格可行的, 则$x^*,\alpha^*,\beta^*$是原始问题和对偶问题的解的充分必要条件是他们满足Karush-Kuhn-Tucker条件
$$
\begin{align}
&\nabla_xL(x^*,\alpha^*,\beta^*)=0\\
&\alpha_i^*c_i(x^*)=0\quad i=1,2,...,k\\
&c_i(x^*)\leq0\quad i = 1,2,...,k\\
&\alpha^*_i\ge0\quad i = 1,2,...,k\\
&h_j(x^*)=0\quad j = 1,2,...,l
\end{align}
$$
若$\alpha_i^*>0$则$c_i^*=0$

### 对SVM约束优化问题引入对偶算法

原问题:
$$
\begin{align}
& \min_{w, b} \frac{1}{2} \|w\|^2\\
& s.t.\ y_i(w\cdot x_i + b) - 1\ge 0
\end{align}
$$
构造拉格朗日函数
$$
L(w,b,\alpha) = \frac{1}{2}\|w\|^2 -\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b) +\sum_{i=1}^N\alpha_i
$$
其中$\alpha = (\alpha_1,\alpha_2,...,\alpha_N)^T$为拉格朗日乘子向量, 原问题的对偶问题为
$$
\max_\alpha\min_{w,b}L(w,b,a)
$$

### 最小化问题

对$w,b$求偏导数, 并另其为0
$$
\begin{align}
&\nabla_wL(w,b,a) = w-\sum_{i=1}^{N}\alpha_iy_ix_i = 0\\
& w = \sum_{i=1}^N\alpha_iy_ix_i\\
&\nabla_bL(w,b,a) = -\sum_{i= 1}^N \alpha_iy_i = 0\\
&\sum_{i=1}^N{a_iy_i} = 0
\end{align}
$$
带入得到, 注意负号
$$
\min_{w,b} = -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i =1}^N \alpha_i
$$

### 最大化问题

$$
\begin{align}
&\max_\alpha -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) +\sum_{i=1}^N\alpha_i\\
&s.t.\begin{cases}
&\sum_{i=1}^N{\alpha_iy_i} = 0\\&\alpha_i\ge0\ i = 1,2,...,N\end{cases}
\end{align}
$$

可以通过加负号转化为最小问题
$$
\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) -\sum_{i=1}^N\alpha_i
$$

注意: 原优化问题满足凸函数/不等式严格可行, 所以根据上面对偶定理3存在一组参数,为对偶和原始问题的解, 因此可以转化为求对偶问题. 如果解存在, 那么充分必要条件是满足KKT条件, 因此带入KKT条件得:
$$
\begin{align}
&\nabla_xL(w^*,b^*,\alpha^*) = w^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0\\
&\nabla_{b}L(w^*,b,\alpha^*)=-\sum_{i=1}^N\alpha_i^*y_i=0\\
&\alpha_i^*(y_i(w^*\cdot x_i + b^*) - 1) = 0\quad i=1,2,...,N \\
&y_i(w^*\cdot x_i + b^*) - 1\ge 0 \quad i = 1,2,...,N\\
&\alpha_i^*\ge 0 \quad i = 1, 2, ...,N
\end{align}
$$
可以得到
$$
w^*=\sum_{i=1}^N(\alpha_i^*y_ix_i)
$$
其中,至少有一个$\alpha_i^*>0$, 如果$\alpha$全为0, $w$为0, 原始超平面$(0, b)$不能将样本二分类, 因此矛盾, 因此存在$\alpha$ 大于0, 设$\alpha_j^*>0$
$$
y_j*(w^*\cdot x_j + b^* ) - 1= 0
$$
将$w^*$带入
$$
\begin{align}
y_j*\left(\sum_{i=1}^N\alpha_i^*y_ix_i\cdot x_j+b^*\right) - 1 &= 0\\
y_j * y_j &= 1\\
\sum_{i=1}^N\alpha_i^*y_ix_i\cdot x_j+b^*&=y_i\\
y_j - \sum_{i=1}^N\alpha^*_iy_i(x_i\cdot x_j)&=b^*
\end{align}
$$
同理, 分离超平面$w^*x+b^*=0$, 带入$w^*$
$$
\begin{align}
&\sum_{i=1}^N \alpha_i^*y_i(x\cdot x_i) + b^*=0\\
&b^* = y_j - \sum_{i=1}^N\alpha^*_iy_i(x_i\cdot x_j)\\
&f(x)  = \text{sign}\left(\sum_{i=1}^N\alpha_i^*y_i(x\cdot x_i) + b^*\right)
\end{align}
$$
分类决策函数只依赖于输入$x$和样本的內积, 且$w^*$ $b^*$全部可以通过$\alpha$求解, 因此只需求解$\alpha$. 引入学习算法, 指出求解顺序

## 线性可分支持向量机学习算法

输入: 线性可分训练集$T=\{(x_1,y_1), ...,(x_N,y_N)\}$, 其中$x_i\in R^n$,$y)i\in\{-1, +1\}$

输出: 分离超平面和决策函数

(1) 构造并求解约束最优化问题
$$
\begin{align}
&\min_\alpha \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i
\\
&s.t.\begin{cases}&
\sum_{i=1}^N\alpha_iy_i=0\\&\alpha_i\ge0\quad i = 1,2,...,N\end{cases}
\end{align}
$$


求解最优解$\alpha^*=(\alpha_1^*, \alpha_2^*,...,\alpha_N^*)^T$

(2) 计算
$$
w^*=\sum_{i=1}^N\alpha_i^*y_ix_i
$$
选择$\alpha^*$的一个正分量计算:
$$
b^*=y_i = \sum_{i=1}^N\alpha^*y_i(x_i\cdot x_j)
$$
(2) 得到分离超平面和决策函数
$$
w^*\cdot x + b^* = 0\\
f(x) = \text{sign}(w^*\cdot x + b^*)
$$
将训练数据中对应$\alpha_i^*>0$的实例点成称为支持向量



## 软间隔最大化

训练样本中可能存在噪声或者特异点, 可能导致不等式约束不能都成立, 修改硬间隔最大化, 定义软间隔如下:
$$
y_i(w\cdot x_i + b) +\xi_i \ge 1
$$
$\xi$ 为松弛变量, 针对 松弛变量, 需要支付地阿基啊$\xi_i$, 因此修改目标函数为,$C$为惩罚系数. 表示对误分类的惩罚成都
$$
\frac{1}{2}\|w\|^2+C\sum_{i=1}^N\xi_i
$$
因此原始问题被定义为
$$
\begin{align}
&\min_{w,b,\xi} \frac{1}{2}\|w\|^2+C\sum_{i=1}^N\xi_i
\\
&s.t. \begin{cases}y_i(w\cdot x_i + b)\ge1-\xi_i\quad i = 1, 2,...,N\\
\xi_i\ge 0\quad i = 1, 2, ...,N\end{cases}
\end{align}
$$
应用拉格朗日函数
$$
L(w,b,\xi,\alpha,\mu)=\frac{1}{2}\|w\|^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_1 + b) -1 +\xi_i)-\sum_{i=1}^N\mu_i\xi_i
$$
其中 $\alpha_i\ge0,\mu_i\ge0$

首先求极小问题: 对$w,b,\xi$求偏导数
$$
\begin{align}
&\nabla_wL(w,b,\xi,\alpha,\mu) = w - \sum_{i=1}^N\alpha_iy_ix_i=0\\
&\nabla_bL(w,b,\xi,\alpha,\mu) = -\sum_{i=1}^N\alpha_iy_i=0\\
&\nabla_{\xi_i}L(w,b,\xi,\alpha,mu) = C - \alpha_i-\mu_i=0
\end{align}
$$
带入原式得
$$
\min_{w,b,\xi}L(w,b,\xi,\alpha,\mu) = -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i
$$
再求极大, 得对偶问题
$$
\begin{align}
& \max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) +\sum_{i=1}^N\alpha_i\\
&s.t.\begin{cases}
&\sum_{i=1}^N\alpha_iy_i=0\\
&C-\alpha_i-\mu_i=0\\
&\alpha_i\ge0\\
&\mu_i\ge 0\quad i = 1,2,...,N
\end{cases}
\end{align}
$$
可以利用$C-\alpha_i- \mu_i=0$消去$\mu$

在解出$\alpha^*$后, 容易求得$w^*$, 主要是如何求解$b^*$,

首先原问题也是凸二次规划问题, 解满足KKT条件, 即
$$
\begin{align}
&\nabla_{w}L(w^*,b^*,\xi^*,\alpha^*,\mu^*) = w^*-\sum_{i=1}^N\alpha_i^*y_ix_i\\
&\nabla_{b}L(w^*,b^*,\xi^*,\alpha^*,\mu^*)=-\sum_{i=1}^N\alpha^*_iy_i=0\\
&\nabla_\xi L(w^*,b^*,\xi^*,\alpha^*\mu^*)=C-\alpha^*-\mu^*=0\\
&\alpha_i^*(y_i(w^*\cdot x_i + b^*) - 1 +\xi_i) = 0\\
&\mu^*_u\xi^*_i=0\\
&y_i(w^*\cdot x_i + b^*) - 1 + \xi^*_i\ge 0\\
&\xi^*\ge0,\mu^*\ge0,\alpha^*\ge0

\end{align}
$$




求解$b^*$, 另$\alpha^*_j>0$, 有
$$
y_j(w^*x_j + b^*)  - 1 + \xi_i =0
$$
同乘$y_j$
$$
b^* =y_j -\xi_i y_j-w^*x_j
$$
由于$\mu_j^*\xi_j^*=0$, 且$ C-\alpha_j^*-\mu_j^*=0$, 存在$\mu_j!=0$, 因而$\xi_j=0$, 所以
$$
b^*=y_j - w^*x_j
$$

## 线性支持向量机学习算法

与线性可分支持向量机学习算法类似

(1) 选择惩罚函数, 构造求解凸二次规划问题
$$
\begin{align}
&\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i\\
&s.t.\begin{cases}
&\sum_{i=1}^N \alpha_iy_i=0\\
&0\le\alpha_i\le C\quad i = 1,2,...,N
\end{cases}

\end{align}
$$
最优解$\alpha^*=(\alpha^*_1,...,\alpha_N^*)^T$

(2) 计算$w^*$
$$
w^*=\sum_{i=1}^{N}\alpha_i^*y_ix_i
$$
选择$\alpha^*$合适的粉分量, 另$0<\alpha_j^*<C$, 计算$b^*$
$$
b^*= y_j - \sum_{i=1}^Ny_i\alpha_i^*(x_i\cdot x_j)
$$
(3) 求得分离超平面和决策函数

注意理论上$b^*$根据支持向量的选择可能不唯一

### 合页(Hinge)损失函数解释

Hinge损失函数被定义为
$$
[z]_+=\begin{cases}z&z>0\\0&z\le0\end{cases}
$$
所以原优化问题等价于
$$
\min_{w,b}\sum_{i=1}^N\left[1- y_i(w\cdot x_i + b)\right]_+ =\lambda\|w\|^2
$$
合页损失在分类正确且置信度高时才为0.



## 非线性SVM

