# 深度学习中需要的矩阵计算

> The Matrix Calculus You Need For Deep Learning
> 翻译自:  [explained.ai](http://explained.ai/)
> 原作者: [Terence Parr](http://parrt.cs.usfca.edu/) and [Jeremy Howard](http://www.fast.ai/about/#jeremy)
>
> 我只翻译了主要的几个部分。翻译有问题请联系我 lih627@outlook.com

[toc]

## 摘要

这篇文章的目的是为了解释深度神经网络训练过程中的矩阵运算。目的是帮助对了解基本神经网络的人加深对其中数学知识的理解。文章末尾提供[参考文献](## 参考文献)总结了文章中讨论的矩阵运算法则。同时可以在 [Theory category at forums.fast.ai](http://forums.fast.ai/c/theory). 探讨文章中的一些理论知识。

## 介绍

机器学习论文和实际软件操作过程比如 [PyTorch](http://pytorch.org/) 存在很大的差异。因为后者通过内置的自动微分功能隐藏了很多细节，如果想要了解最新的训练技术以及实现的底层，需要了解矩阵计算，这包括线性代数[linear algebra](https://en.wikipedia.org/wiki/Linear_algebra)和多元计算[multivariate calculus](https://en.wikipedia.org/wiki/Multivariable_calculus)。

例如，一个简单的神经网络计算单元首先计算权重向量$\mathbb{w}$和输入向量$\mathbb{x}$的点积，并加上一个标量(偏置)。公式为 $z(\mathbb{x})=\sum_{i}^nw_ix_i+b=\mathbb{w}\cdot\mathbb{x}+b$ 通常该函数被称为为仿射函数(affine function)。然后紧接着一个线性整流单元[rectified linear unit](https://goo.gl/7BXceK)，它将负值裁剪为0：$\max(0, z(\mathbb{x}))$。这个计算过程为定义为一个「人工神经元」。神经网络包含多个这样的单元。多个单元聚合成网络层，上一层的输出是下一层的输入。最后一层的输出称作神经网络的输出。

训练一个神神经网络意味着选择合适的权重向量 $\mathbb{w}$ 和偏置 $b$，对于输入的所有向量$\mathbb{x}$都会输出想要的结果。为此会设计一个损失函数，来对比所有输入网络推理的结果$activation(\mathbb{x})$ 和预期结果 $target(\mathbb{x})$ 的差异。为了最小化差异，通常使用SGD或者Adam等梯度下降方法训练。所有这些都需要$activation(\mathbb{x})$对于模型参数$\mathbb{w}$和$b$的偏导数。通过逐步调整$\mathbb{w}$和$b$使得总的损失函数值变得越来越小。

例如可以写一个标量版本的均方误差损失函数：

$$
\frac{1}{N}\sum_{\mathbb{x}}\left(target(\mathbf{x}) - activation(\mathbb{x})\right)^2=\frac{1}{N}\sum_{\mathbb{x}}\left(target(\mathbb{x}) - \max(0, \sum_{1}^{|x|}w_ix_i+b)\right)^2
$$

$|x|$表示向量$x$中元素的个数， 注意这只是一个神经元，神经网络需要同时训练所有层的所有神经元。由于有多个输入和多个网络输出，通常需要一些向量对向量和求导法则。这篇文章的目的于此。

## 复习：标量求导法则

对于标量的求导法则如下：


| Rule | $f(x)$ | 对 $x$ 导数 | 例子  |
| :----: | :----: | :----: | :----: |
|常量 | $c$ | 0 | $\frac{d}{dx}99=0$ |
| 与常量乘 | $cf$ | $c\frac{df}{dx}$ | $\frac{d}{dx}3x=3$ |
| 幂 | $x^n$ | $nx^{n-1}$ | $\frac{d}{dx}x^3=3x^2$ |
| 和 | $f+g$ | $\frac{df}{dx}+\frac{dg}{dx}$ | $\frac{d}{dx}(x^2+3x)=2x+3$ |
| 积 | $fg$ | $f\frac{dg}{dx}+g\frac{df}{dx}$ | $\frac{d}{dx}x^2x=x^2+x2x=3x^2$ |
| 链式法则 | $f(g(x))$ | $\frac{df(u)}{du}\frac{du}{dx}, u= g(x)$ | $\frac{d}{dx}\ln(x^2)=\frac{1}{x^2}2x=\frac{2}{x}$ |

## 向量计算和偏导数

神经网络层并不是由一个参数和一个方程构成的。首先考虑多个参数的情况，例如$f(x,y) = 3x^2y$。此时，$f(x, y)$ 的变化取决于更改 $x$ 还是更改 $y$ 。引出偏导(partial derivatives)。例如对于 $x$ 偏导可以写为$\frac{\partial}{\partial x}3yx^2$ 将 $y$ 看作常量，可以得到偏导结果为 $\frac{\partial}{\partial x}3yx^2=3y\frac{\partial}{\partial x}x^2=6yx$

从整体来看，当考虑整个向量的计算而不是多元函数的计算。对于$f(x, y)$ 需要计算 $\frac{\partial}{\partial x}f(x, y)$ 和 $\frac{\partial}{\partial y}f(x, y)$ 。可以将他们组合成水平向量，因此定义$f(x, y)$ 的导数为：

$$
\nabla f(x, y) = \left[\frac{\partial f(x, y)}{\partial x}, \frac{\partial f(x, y)}{\partial y}\right] = \left[6yx, 3x^2\right]
$$

因此一个多元函数的导数为偏导构成的向量。 他们将 $n$ 个标量参数映射到一个标量。下一节将讨论如何处理多个多元函数的导数。

## 矩阵计算

当从一个多元函数考虑到多个多元函数的导数时，需要从向量运算考虑到矩阵运算。现在考虑两个函数的偏导，例如$f(x, y) = 3x^2y$ 和 $g(x, y) = 2x + y^8$ 。可以分别计算他们的梯度向量并堆叠在一次。此时这个矩阵称为 **Jacobian** ( *Jacobian matrix*) ，如下

$$
J =\begin{bmatrix}\nabla f(x, y)\\
\nabla g(x, y)\end{bmatrix} = 
\begin{bmatrix}
\frac{\partial f(x, y)}{\partial x}&\frac{\partial f(x, y)}{\partial y}\\
\frac{\partial g(x, y)}{\partial x}&\frac{\partial g(x, y)}{\partial y} 
\end{bmatrix}= 
\begin{bmatrix}
6yx & 3x^2\\2&8y^7
\end{bmatrix}
$$

这种形式为分子布局([numerator layout](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)) 对应另外一种为分母布局(denominat layout), 是对上述矩阵的转置。

## Jacobian 矩阵生成

第一步，将多个标量通过向量表示。$f(x, y, z) \to f(\mathbb{x})$。定义默认情况下向量为列向量$n\times 1$，即

$$
\mathbb{x}=
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix}
$$

当有多个结果是标量的函数组合起来，通过 $\mathbb{y}=\mathbb{f(x)}$ 表示。其中 $\mathbb{y}$ 是一个向量表示 $m$ 个结果为标量的方程，每个方程输入元素个数为$n=|\mathbb{x}|$ 的向量。展开如下：

$$
\begin{aligned}
y_1 &= f_1(\mathbb{x})\\
y_2 &= f_2(\mathbb{x})\\
 &\vdots\\
 y_m &=f_m(\mathbb{x})
\end{aligned}
$$

 例如上一节的公式可以用$x_1,x_2$分别表示$x, y$:

$$
\begin{aligned}
y_1 &= f_1(\mathbb{x}) =3x_1^2x_2\\
y_2 &= f_2(\mathbb{x}) = 2x_1 + x_2^8
\end{aligned}
$$

通常Jacobian矩阵包含所有$m\times n$ 个偏导。它记录了$m$个结果是标量函数对于 $\mathbb{x}$ 的梯度

$$
\frac{\partial\mathbb{y}}{\partial\mathbb{x}}=
\begin{bmatrix}
\nabla f_1(\mathbb{x})\\
\nabla f_2(\mathbb{x})\\
\vdots \\
\nabla f_m(\mathbb{x})
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial}{\partial\mathbb{x}}f_1(\mathbb{x})\\
\frac{\partial}{\partial\mathbb{x}}f_2(\mathbb{x})\\
\vdots\\
\frac{\partial}{\partial\mathbb{x}}f_m(\mathbb{x})
\end{bmatrix}=
\begin{bmatrix}
\frac{\partial}{\partial x_1}f_1(\mathbb{x})
& \frac{\partial}{\partial x_2}f_1(\mathbb{x})
&\cdots &\frac{\partial}{\partial x_n}f_1(\mathbb{x})\\
\frac{\partial}{\partial x_1}f_2(\mathbb{x})
& \frac{\partial}{\partial x_2}f_2(\mathbb{x})
&\cdots
& \frac{\partial}{\partial x_n} f_2(\mathbb{x})\\
\vdots &\vdots&&\vdots\\
\frac{\partial}{\partial x_1}f_m(\mathbb{x})
&\frac{\partial}{\partial x_2}f_m(\mathbb{x})
&\cdots
&\frac{\partial}{\partial x_n}f_m(\mathbb{x})
\end{bmatrix}
$$

每个$\frac{\partial}{\partial \mathbb{x}}f_i(\mathbb{x})$ 是一个$n=|\mathbb{x}|$ 元水平向量。

下面考虑恒等式$\mathbb{f(x) = x}$, $f_i(\mathbf{x})=\mathbf{x}$ ，该恒等式包含 $n$ 个函数每个函数包含 $n$ 个参数，那么其 Jacobian矩阵是一个方阵 $m=n$。

$$
\begin{aligned}
\frac{\partial\mathbf{b}}{\partial\mathbf{x}}=
\begin{bmatrix}
\frac{\partial}{\partial\mathbf{x}}f_1(\mathbf{x})\\
\frac{\partial}{\partial\mathbf{x}}f_2(\mathbf{x})\\
\vdots\\
\frac{\partial}{\partial\mathbf{x}}f_m(\mathbf{x})
\end{bmatrix}
&=
\begin{bmatrix}
\frac{\partial}{\partial x_1}f_1(\mathbb{x})
& \frac{\partial}{\partial x_2}f_1(\mathbb{x})
&\cdots &\frac{\partial}{\partial x_n}f_1(\mathbb{x})\\
\frac{\partial}{\partial x_1}f_2(\mathbb{x})
& \frac{\partial}{\partial x_2}f_2(\mathbb{x})
&\cdots
& \frac{\partial}{\partial x_n} f_2(\mathbb{x})\\
\vdots &\vdots&&\vdots\\
\frac{\partial}{\partial x_1}f_m(\mathbb{x})
&\frac{\partial}{\partial x_2}f_m(\mathbb{x})
&\cdots
&\frac{\partial}{\partial x_n}f_m(\mathbb{x})
\end{bmatrix}\\
&=
\begin{bmatrix}
\frac{\partial}{\partial x_1}x_1
& \frac{\partial}{\partial x_2}x_1
&\cdots &\frac{\partial}{\partial x_n}x_1\\
\frac{\partial}{\partial x_1}x_2
& \frac{\partial}{\partial x_2}x_2
&\cdots
& \frac{\partial}{\partial x_n} x_2
\\
\vdots &\vdots&&\vdots\\
\frac{\partial}{\partial x_1}x_n
&\frac{\partial}{\partial x_2}x_n
&\cdots
&\frac{\partial}{\partial x_n}x_n
\end{bmatrix}\\
&=I
\end{aligned}
$$


## 向量元素级二元运算符的导数

很多向量复杂计算可以简化为多个元素级向量二元操作的组合。例如通过 $\mathbf{y}=\mathbf{f(w)}\bigcirc\mathbf{g(w)} $其中$m = n=|\mathbb{y}|=|\mathbb{w}|=|\mathbb{x}|$，例如下面这个例子

$$
\begin{bmatrix}
y_1\\ y2\\ \vdots\\y_n
\end{bmatrix}
=
\begin{bmatrix}
f_1(\mathbf{w})\bigcirc g_1(\mathbf{x})\\
f_2(\mathbf{w})\bigcirc g_2(\mathbf{x})\\
\vdots\\
f_n(\mathbf{w})\bigcirc g_n(\mathbf{x})
\end{bmatrix}
$$

考虑对$\mathbf{x}$的Jacobian

$$
J_\mathbf{w}=\frac{\partial\mathbf{y}}{\partial\mathbf{w}}=
\begin{bmatrix}
\frac{\partial}{\partial w_1}\left(f_1(\mathbf{w})\bigcirc g_1(\mathbf{x})\right)
&\frac{\partial}{\partial w_2}\left(f_1(\mathbf{w})\bigcirc g_1(\mathbf{x})\right)
&\cdots
&\frac{\partial}{\partial w_n}\left(f_1(\mathbf{w})\bigcirc g_1(\mathbf{x})\right)\\
\frac{\partial}{\partial w_1}\left(f_2(\mathbf{w})\bigcirc g_2(\mathbf{x})\right)
&\frac{\partial}{\partial w_2}\left(f_2(\mathbf{w})\bigcirc g_2(\mathbf{x})\right)
&\cdots
&\frac{\partial}{\partial w_n}\left(f_2(\mathbf{w})\bigcirc g_2(\mathbf{x})\right)\\
\vdots & \vdots &&\vdots\\
\frac{\partial}{\partial w_1}\left(f_n(\mathbf{w})\bigcirc g_n(\mathbf{x})\right)
&\frac{\partial}{\partial w_2}\left(f_n(\mathbf{w})\bigcirc g_n(\mathbf{x})\right)
&\cdots
&\frac{\partial}{\partial w_n}\left(f_n(\mathbf{w})\bigcirc g_n(\mathbf{x})\right)\\
\end{bmatrix}
$$

上面的式子看起来比较麻烦，先考虑Jacobian是一个对角矩阵，即$\frac{\partial}{\partial w_j}\left(f_i(\mathbf{w})\bigcirc g_i(\mathbf{x})\right) = 0, i\neq j$ 。此时非对角线上的元素恒为0。 $f_i$ 只是一个和$w_i$ 有关的函数，此时二元表达式可以简化为 $f_i(w_i)\bigcirc g_i(x_i)$， Jacobian可以表示为:

$$
\frac{\partial \mathbf{y}}{\partial\mathbf{w}}=diag\left(\frac{\partial}{\partial w_1}(f_1(w_1)\bigcirc g_1(x_1)),
\frac{\partial}{\partial w_2}(f_2(w_2)\bigcirc g_2(x_2)),
\cdots,
\frac{\partial}{\partial w_n}(f_n(w_n)\bigcirc g_n(x_n))\right)
$$

对应偏导运算可以总结如下

| Op        | Partial with Respect to $\mathbf{w}$                         |
| --------- | ------------------------------------------------------------ |
| $+$       | $\frac{\partial(\mathbf{w} + \mathbf{x})}{\partial\mathbf{w}}=diag(\cdots\frac{\partial(w_i + x_i)}{\partial w_i}\cdots)=I$ |
| $-$       | $\frac{\partial(\mathbf{w} + \mathbf{x})}{\partial\mathbf{w}}=diag(\cdots\frac{\partial{(w_i - x_i)}}{\partial w_i}\cdots)=I$ |
| $\otimes$ | $\frac{\partial(\mathbf{w}\otimes\mathbf{x})}{\partial\mathbf{w}}=diag\left(\cdots\frac{\partial(w_i\times x_i)}{\partial w_i}\cdots\right)=diag(\mathbf{x})$ |
| $\oslash$ | $\frac{\partial(\mathbf{w}\oslash\mathbf{x})}{\partial\mathbf{w}}=diag\left(\cdots\frac{\partial(w_i/ x_i)}{\partial w_i}\cdots\right)=diag(\cdots\frac{1}{x_i}\cdots)$|

对 $\mathbf{x}$ 的偏导

| OP        | Partial With Respect to $\mathbf{x}$                         |
| --------- | ------------------------------------------------------------ |
| $+$       | $\frac{\partial(\mathbf{w+x})}{\partial\mathbf{x}}=I$        |
| $-$       | $\frac{\partial(\mathbf{w-x})}{\partial\mathbf{x}}=-I$       |
| $\otimes$ | $\frac{\partial(\mathbf{w\otimes x})}{\partial\mathbf{x}}=diag(\mathbf{w})$ |
| $\oslash$ | $\frac{\partial(\mathbf{w\oslash x})}{\partial\mathbf{x}}=diag\left(\cdots\frac{-w_i}{x_i^2}\cdots\right)$ |

其中 $\otimes$ 和 $\oslash$ 表示元素间乘除法。

## 涉及标量运算的导数

当使用标量和向量之间的运算，例如乘法或者加法时，可以把标量拓展为向量并表示为两个向量之前二元运算。例如，将标量 $z$ 与向量 $\mathbf{x}$ 相加 $\mathbf{y} = \mathbf{x} + z=f(\mathbf{x}) + g(z)$ 此时 $f(\mathbf{x})=\mathbf{x}$ , $g(z) = \vec{1}z$ 。同理  $\mathbf{y}=\mathbf{x}z=\mathbf{x}\otimes\vec{1}z$ 。 此时可以通过上一节的内容来计算导数。

$\frac{\partial\mathbf{y}}{\partial\mathbf{x}}=diag\left(\cdots \frac{\partial}{\partial}(f_i(x_i)\bigcirc g_i(z))\cdots\right)$

对应可得

$$
\frac{\partial}{\partial\mathbf{x}}(\mathbf{x} + z) = diag(\vec{1})= I\\
\frac{\partial}{\partial z}(\mathbf{x} + z) = diag(\vec{1})= I
$$

$$
\frac{\partial}{\partial\mathbf{x}}(\mathbf{x}z)=diag(\vec{1}z) = Iz\\
\frac{\partial}{\partial z}(\mathbf{x}z)= \mathbf{x}
$$

关于最后一个等式推导如下, $x$ 是一个列向量： 

$$
\frac{\partial}{\partial z}(f_i(x_i)\otimes g_i(z) ) = x_i\frac{\partial z}{\partial z} + z\frac{\partial x_i}{\partial z} = x_i + 0 = x_i
$$

> 向量对向量求导：结果是矩阵
>
> 向量对标量求导：结果是向量

## 向量归约和(sum reduction)

深度学习通常会计算向量中所有元素的和，例如网络的损失函数。可以通过向量点乘或者其他操作把向量转化为标量。

令 $y=\sum(\mathbf{f(x)}) = \sum_{i=1}^nf_i(\mathbf{x})$ ，注意每个函数的参数都是向量 $\mathbf{x}$ 。对应雅可比矩阵为 $1\times n$ 向量:

$$
\begin{aligned}
\frac{\partial y}{\partial\mathbf{x}}&=
\begin{bmatrix}
\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \cdots,\frac{\partial y}{\partial x_n}
\end{bmatrix}\\
&=
\begin{bmatrix}
\frac{\partial}{\partial x_1}\sum_i f_i(\mathbf{x}), \frac{\partial}{\partial x_2}\sum_if_i(\mathbf{x}),\cdots,\frac{\partial}{\partial x_n}\sum_if_i(\mathbf{x})
\end{bmatrix}\\
&=
\begin{bmatrix}
\sum_i\frac{\partial f_i(\mathbf{x})}{\partial x_1}, \sum_i\frac{\partial f_i(\mathbf{x})}{\partial x_2},\cdots,\sum_i\frac{\partial f_i(\mathbf{x})}{\partial x_n}
\end{bmatrix} (\text{move derivate inside} \sum)
\end{aligned}
$$
考虑最简单的情况 $y = sum(\mathbf{x})$ , 此时 $f_i(\mathbf{x})=x_i$
$$
\nabla y = 
\begin{bmatrix}
\sum_i\frac{\partial x_i}{\partial x_1},\sum_i\frac{\partial x_i}{\partial x_2},\cdots,\sum_i\frac{\partial x_i}{\partial x_n}
\end{bmatrix} = [1, 1,\cdots,1] = \vec{1}^T
$$

此时结果是一个全1的行向量。

考虑另外一种情况: $y= sum(\mathbf{x}z)$ , $f_i(\mathbf{x}, z)=x_iz$, 梯度为

$$
\begin{aligned}
\frac{\partial y}{\partial \mathbf{x}} &=
\begin{bmatrix}
\sum_i\frac{\partial}{\partial x_1}x_iz,\sum_i\frac{\partial}{\partial x_2}x_iz, \cdots, \sum_i\frac{\partial}{\partial x_n}x_iz
\end{bmatrix}\\
&=
\begin{bmatrix}
z, z,\cdots, z
\end{bmatrix}
\end{aligned}
$$

现在考虑对于标量 $z$ 的梯度，其结果是$1\times 1$标量

$$
\begin{aligned}
\frac{\partial\mathbf{y}}{\partial z} &= \frac{\partial}{\partial z}\sum_{i=1}^n x_iz\\
&= \sum_i\frac{\partial}{\partial z}x_i z\\
&= \sum_ix_i\\
&=sum(\mathbf{x})
\end{aligned}
$$

## 链式法则

从上面可以知道，复杂的函数计算可以通过基本的矩阵运算来实现。例如通常不能够直接计算嵌套表达式的梯度如 $sum(\mathbf{w + x})$ (除非将它门展开为标量计算)。 但是可以通过链式法则组合基本的矩阵求导法则来计算。下面先举例解释单变量链式法则(single-variable chain rule)。即标量方程对标量求导。进而推广到全导数(total derivative)并且使用它去定义单变量全导数链式法则(single-variable total-derivative chain rule)。它在神经网络中受到广泛的应用。

### Single-variable chain rule

链式法则也是一种分而治之的策略。将复杂的表达式分解为子表达式，且子表达式的导数更方便求解。例如求解 $\frac{d}{dx}sin(x^2)=2xcos(x^2)$ 外层的$sin$ 可以使用内层表达式的结果。$\frac{d}{dx}x^2 = 2x$ , $\frac{d}{du}sin(u)=cos(u)$ 。看起来像是外部函数的导数和内部函数的导数链接起来。通常复合函数可以被写作$y=f(g(x))$ 或者 $(f\circ g)(x)$ 。$y = f(u), u= g(x)$ 链式法则可以表示为


$$
\frac{dy}{dx} = \frac{dy}{du}\frac{du}{dx}
$$


1. 通过中间变量把复杂函数求导转化为两个简单函数的求导
2. 分别计算两个简单函数的导数
3. 两个导数结果想乘
4. 替换中间变量

链式法则也可以通过数据流或者抽象语法树([abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree))表示

<img src="https://raw.githubusercontent.com/lih627/MyPicGo/master/imgs/20200928104538.png" alt="abstract syntax tree" style="zoom:20%;" />

如上图所示，更改参数 $x$ 会通过平方和正弦函数作用于 $y$ 。可以把 $\frac{du}{dx}$ 理解为$x$ 的变化传导到$u$。 链式法则可以表示为 $\frac{dy}{dx}=\frac{du}{dx}\frac{dy}{du}$ (x到y)

**Single-variable chain rule 应用场景**：注意上图 $x$ 到 $y$ 只有一条数据流。因此 $x$  的改变仅能通过一条路径影响到 $y$ 。 但是如果表达式为  $y(x) = x + x^2$ ，它表达为 $y(x, u) = x + u，此时$ $y(x, u)$ 的数据流图有多条路径，此时应该使用单变量全微分链式法则(single-variable total-derivative chain rule)。可以先考虑下面这个式子 $y = f(x)=ln(sin(x^3)^2)$， 过程如下：

1. 使用中间变量
   $$
   \begin{aligned}
   u_1 &= f_1(x) = x^3\\
   u_2 &= f_2(u_1)= sin(u_1)\\
   u_3 &= f_3(u_2) = u_2^2\\
   u_4 &= f_4(u_3) =ln(u_3)(y = u_4)
   \end{aligned}
   $$

2. 计算微分
   $$
   \begin{aligned}
   \frac{d}{du_x}u_1 &= 3 x^2\\
   \frac{d}{du_1}u_2&= cos(u_1)\\
   \frac{d}{du_2}u_3 &= 2u_2\\
   \frac{d}{du_3}u_5 &= \frac{1}{u_3}
   \end{aligned}
   $$

3. 组合四个中间变量
   $$
   \frac{dy}{dx} = \frac{du_4}{dx} = \frac{1}{u_3}2u_2cos(u_1)3x^2 = \frac{6u_2x^2cos(u_1)}{u_3}
   $$

4. 替换中间变量
   $$
   \frac{dy}{dx} = \frac{6sin(u_1)x^2cos(x^3)}{u_2^2} = \frac{6sin(x^3)x^2cos(x^3)}{sin(x^3)^2} = \frac{6x^2cos(x^3)}{sin(x^3)}
   $$


可视化链式法则如下图(1条路径)

<img src="https://raw.githubusercontent.com/lih627/MyPicGo/master/imgs/20200928114526.png" alt="visualization chain rule" style="zoom:20%;" />

### Single-variable total-derivative chain rule

单变量链式法则应用范围有限，因为每个中心变量都必须是单变量的函数。但是它展示了链式法则的核心。如果想要对$y=f(x)=x + x^2$ 通过链式法则求导，需要对基本的链式法则做增强。

显然可以直接求导$\frac{dy}{dx}=\frac{d}{dx}x + \frac{d}{dx}x^2 = 1 + 2x$。但是它应用了变量加法的导数法则而不是链式法则。先尝试用链式法则来计算


$$
\begin{aligned}
u_1(x) &= x^2\\
u_2(x, u_1) &=x + u _1\quad(y=f(x)=u_2(x, u_1))
\end{aligned}
$$


先假设 $\frac{du_2}{du_1} = 0 + 1 = 1$  和 $\frac{du_1}{dx}=2x$ , 则 $\frac{dy}{dx} = \frac{du_2}{dx}=\frac{du_2}{du_1}\frac{du_1}{dx} = 2x$ y与正确结果不相同。原因在于 $u_2(x, u) = x + u_1$ 有多个参数，此时需要引入偏导数。先尝试一下：


$$
\begin{aligned}
\frac{\partial u_1(x)}{\partial x} &= 2x\\
\frac{\partial u_2(x,u_1)}{\partial u_1} &= \frac{\partial}{\partial u_1}(x + u_1) = 0 + 1= 1\\
\frac{\partial u_2(x, u_1)}{\partial x}&\neq \frac{\partial}{\partial x}(x + u_1) = 1 + 0 = 1
\end{aligned}
$$


$\frac{\partial u_2(x, u_1)}{\partial x}$ 出现问题，因为$u_1$ 包含变量了$x$。在计算偏导的时候不能把 $u_1$ 看作标量。可以通过如下计算图展示。

<img src="https://raw.githubusercontent.com/lih627/MyPicGo/master/imgs/20200928172646.png" alt="compute node" style="zoom:20%;" />

$x$ 的变化会通过加法和平方运算影响到 $y$。下面的式子可以看出来 $x$ 如何影响 $y$


$$
\hat{y}  = (x +\Delta x) + (x +\Delta x)^2
$$


$\Delta y = \hat{y} - y$, 此时需要引出总导数( [total derivatives](https://en.wikipedia.org/wiki/Total_derivative)）, 他假设所有的中间变量都包含 $x$ 并且可能随着$x$ 的变化而变化。公式如下：


$$
\frac{dy}{dx}=\frac{\partial f(x)}{x} = \frac{\partial u_2(x, u_1)}{\partial x} = \frac{\partial u_2}{\partial x}\frac{\partial x}{\partial x} + \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x} = \frac{\partial u_2}{\partial x} + \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}
$$


带入公式：
$$
\frac{dy}{dx} = \frac{\partial u_2}{\partial x} + \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x} = 1 + 1\times2x = 1 = 2x
$$


单变量总导数链式法则(single-variable total-derivative chaine rule)总结如下：


$$
\frac{\partial f(x, u_1,\cdots,u_n)}{\partial x}=\frac{\partial f}{\partial x} + \sum_i^n\frac{\partial f}{\partial u_i}\frac{\partial u_i}{\partial x}
$$
下面例子 $f(x) = sin(x + x^2)$


$$
\begin{aligned}
u_1(x) &= x^2\\
u_2(x, u_1) &= x + u_1\\
u_3(u_2) &= sin(u_2)
\end{aligned}
$$


对应偏导


$$
\begin{aligned}
\frac{\partial u_1}{\partial x} &= 2x\\
\frac{\partial u_2}{\partial x} &=\frac{\partial x}{\partial x} + \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}= 1 + 2x\\
\frac{\partial f(x)}{\partial x} &= \frac{\partial u_3}{\partial x} +\frac{\partial u_3}{\partial u_2}\frac{\partial u_2}{\partial x} = 0 + cos(u_2)\frac{\partial u_2}{\partial x} = cos(x + x^2)(1+2x)
\end{aligned}
$$


可以针对$f(x) = x^3$ 应用法则：


$$
\begin{aligned}
u_1(x) &= x^2\\
u_2(x, u1) &= xu_1\\
\frac{\partial u_1}{\partial x} &= 2x\\
\frac{\partial u_2}{\partial x} &= u_1 + \frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x} = x^2 + x\times 2x = 3x^2 
\end{aligned}
$$


使用更多的中间变量，可以把求导分解成更简单的子问题。可以引入  $x:u_{n+1} = x$ 来更为清晰的展示链式法则:


$$
\frac{\partial f(u_1,\cdots, u_{n + 1})}{\partial x} = \sum_{i=1}^{n + 1}\frac{\partial f}{\partial u_i}\frac{\partial u_i}{\partial x}
$$


### 向量链式法则

把标量表达式拓展到向量 $\mathbf{y} = \mathbf{f}(x)$, 例如
$$
\begin{bmatrix}
y_1(x)\\
y_2(x)\\
\end{bmatrix}=
\begin{bmatrix}
f_1(x)\\
f_2(x)
\end{bmatrix}=
\begin{bmatrix}
ln(x^2)\\
sin(3x)
\end{bmatrix}
$$


首先引入两个中间变量 $g_1$ 和 $g_2$ 
$$
\begin{aligned}
\begin{bmatrix}
g_1(x)\\
g_2(x)
\end{bmatrix} &=
\begin{bmatrix}
x^2\\
3x
\end{bmatrix}\\
\begin{bmatrix}
f_1(\mathbf{g})\\
f_2(\mathbf{g})
\end{bmatrix} &=
\begin{bmatrix}
ln(g_1)\\
sin(g_2)
\end{bmatrix}
\end{aligned}
$$
则关于标量 $x$ 的导数构成的向量 $\mathbf{y}$ 是一个列向量，可以通过单变量总导数链式法则计算


$$
\frac{\partial\mathbf{y}}{\partial x} =
\begin{bmatrix}
\frac{\partial f_1(\mathbf{g})}{\partial x}\\
\frac{\partial f_2(\mathbf{g})}{\partial x}
\end{bmatrix}=
\begin{bmatrix}
\frac{\partial f_1}{\partial g_1}\frac{\partial g_1}{\partial x} + \frac{\partial f_1}{\partial g_2}\frac{\partial g_2}{\partial x}\\
\frac{\partial f_2}{\partial g_1}\frac{\partial g_1}{\partial x} + \frac{\partial f_2}{\partial g_2}\frac{\partial g_2}{\partial x}
\end{bmatrix} = 
\begin{bmatrix}
\frac{1}{g_1}2x+0\\
0 + cos(g_2)3
\end{bmatrix}=
\begin{bmatrix}
\frac{2}{x}\\
3cos(3x)
\end{bmatrix}
$$


上个公式表明，可以通过标量的链式法则求导后将其组合为向量，更一般的，可以从下面表达式发现规律


$$
\frac{\partial}{\partial x}\mathbf{f}(g(x))=
\begin{bmatrix}
\frac{\partial f_1}{\partial g_1}\frac{\partial g_1}{\partial x} + \frac{\partial f_1}{\partial g_2}\frac{\partial g_2}{\partial x}\\
\frac{\partial f_2}{\partial g_1}\frac{\partial g_1}{\partial x} + \frac{\partial f_2}{\partial g_2}\frac{\partial g_2}{\partial x} 
\end{bmatrix} =
\begin{bmatrix}
\frac{\partial f_1}{\partial g_1} & \frac{\partial f_1}{\partial g_2}\\
\frac{\partial f_1}{\partial g_1} & \frac{\partial f_2}{\partial g_2}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial g_1}{\partial x}\\
\frac{\partial g_2}{\partial x}
\end{bmatrix}=
\frac{\partial \mathbf{f}}{\partial \mathbf{g}}
\frac{\partial \mathbf{g}}{\partial x}
$$


这说明Jacobian可以通过两个Jacobian乘法运算得到。更一般的，当输入是向量时，只需要把第二个向量用矩阵表示，如下表达式
$$
\frac{\partial}{\partial \mathbf{x}}\mathbf{f(g(x))} =
\frac{\partial \mathbf{f}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial\mathbf{x}} =
\begin{bmatrix}
\frac{f_1}{g_1} &\frac{f_1}{g_2} &\cdots &\frac{f_1}{g_k}\\
\frac{f_2}{g_1} &\frac{f_2}{g_2} &\cdots &\frac{f_2}{g_k}\\
\vdots &\vdots &&\vdots\\
\frac{f_m}{g_1}&\frac{f_m}{g_2} &\cdots&\frac{f_m}{g_k}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial g_1}{\partial x_1} &\frac{\partial g_1}{\partial x_2}&\cdots&\frac{\partial g_1}{\partial x_n}\\
\frac{\partial g_2}{\partial x_1} &\frac{\partial g_2}{\partial x_1}&\cdots&\frac{\partial g_2}{\partial x_n}\\
\vdots&\vdots&&\vdots\\
\frac{\partial g_k}{\partial x_1}& \frac{\partial g_k}{\partial x_2} &\cdots &\frac{\partial g_k}{\partial x_n}
\end{bmatrix}
$$


从单变量链式拓展到向量形式一个直观的好处是，可以通过同样的公式表示总导数。上面的式子中 $m = |\mathbf{f}|, n = |\mathbf{x}|, k = |\mathbf{g}| $  最终Jacobian为 $m\times n$ 矩阵。

即使得到了$\frac{\partial \mathbf{f}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial \mathbf{x}}$ 公式，很多情况下还可以做进一步简化。Jacobian 通常是方阵，并且非对角线元素是 0 。神经网络一般处理关于向量的方程，而不是方程构成的向量。例如，对于仿射函数 $sum(\mathbf{w}\otimes\mathbf{x})$ 和激活函数 $max(0, \mathbf{x})$ 下一节会介绍他的导数。下图给出了Jacobian的形状。(长方形形状表示标量/行向量/列向量/矩阵)

![Jacobian](https://raw.githubusercontent.com/lih627/MyPicGo/master/imgs/20201001001119.png)



## 神经激活函数的梯度

下面了来计算神经网络激活函数的导数，包括参数$\mathbf{w}$  和 $b$ (注意$\mathbf{x} $ 和 $\mathbf{w}$ 都是列向量)：
$$
activation(\mathbf{x})=max(0, \mathbf{w}\cdot\mathbf{x} + b)
$$
上述表示全连接层在接一个线形整流单元作为激活函数。首先忽略$max$ 函数，计算 $\frac{\partial}{\partial\mathbf{w}}(\mathbf{w\cdot x} + b)$  和 $\frac{\partial}{\partial b}(\mathbf{w\cdot x} + b)$ 。首先考虑 $\mathbf{w\cdot x}$ 。其实就是元素对应位置的乘积的和。 即 $\sum_{i}^n(w_ix_i)=sum(\mathbf{w\otimes x})$ 。或者采用线性代数的表达方式 $\mathbf{w\cdot x} = \mathbf{w}^T\mathbf{x}$ 。在上面章节已经讨论过了 $sum(\mathbf{x})$ 和 $\mathbf{w\otimes x	}$ 的偏导数了。在这里使用链式法则：


$$
\begin{aligned}
\mathbf{u} &= \mathbf{w\otimes x}\\
y &= sum(\mathbf{u})
\end{aligned}
$$


计算偏导数：


$$
\begin{aligned}
\frac{\partial\mathbf{u}}{\partial \mathbf{w}} &= \frac{\partial}{\partial \mathbf{w}}(\mathbf{w\otimes x}) = diag(\mathbf{x})\\
\frac{\partial y}{\partial\mathbf{u}} &= \frac{\partial}{\partial \mathbf{u}}sum(\mathbf{u}) =\vec{1}^T 
\end{aligned}
$$
通过链式法则可得：
$$
\frac{\partial y}{\partial \mathbf{w}} = \frac{\partial y}{\partial \mathbf{u}}\frac{\partial \mathbf{u}}{\partial \mathbf{w}} = \vec{1}^Tdiag(\mathbf{x}) = \mathbf{x}^T
$$
因此有:
$$
\frac{\partial y}{\partial \mathbf{w}} = [x_1, \cdots, x_n] = \mathbf{x}^T
$$
 现在考虑 $y=\mathbf{w\cdot x} + b$ 需要考虑两个偏导，并且不需要链式法则：
$$
\begin{aligned}
\frac{\partial y}{\partial \mathbf{w}} &= \frac{\partial}{\partial\mathbf{w}}\mathbf{w\cdot x} + \frac{\partial}{\partial \mathbf{w}}b = \mathbf{x}^T + \vec{0}^T = \mathbf{x}^T\\
\frac{\partial y}{\partial b} &= \frac{\partial}{\partial b} \mathbf{w\cdot x} + \frac{\partial}{\partial b} b = 0 + 1 = 1 
\end{aligned}
$$
下面需要考虑 $max(0, z)$ 函数的导数， 显然
$$
\frac{\partial}{\partial z}max(0, z) =\left\{
\begin{array} {**lr**}
0 &z\le 0\\
\frac{dz}{dz} = 1&z >0 
\end{array}
\right.
$$
当计算加激活函数后的梯度时，使用向量链式法则;
$$
\begin{aligned}
z(\mathbf{w}, b, \mathbf{x}) &= \mathbf{w\cdot x} + b\\
activation(z) &= max(0, z)
\end{aligned}
$$
链式法则表达为:
$$
\frac{\partial activation}{\partial \mathbf{w}} = \frac{\partial activaation}{\partial z} \frac{\partial z}{\partial \mathbf{w}}
$$
带入表达式为:
$$
\frac{\partial activation}{\partial \mathbf{w}} = 
\left\{
\begin{array}{**lr**}
0\frac{\partial z}{\partial w} = \vec{0}^T &
\mathbf{w\cdot x} + b \le 0\\
1\frac{\partial z}{\partial\mathbf{w}}=\mathbf{x}^T & \mathbf{w\cdot x} + b>0
\end{array}
\right.
$$

同理:
$$
\frac{\partial activation}{\partial b} = \left\{
\begin{array}{**lr**}
0\frac{\partial z}{\partial b} = 0 &\mathbf{w\cdot x}  +b \le 0\\
1\frac{\partial z}{\partial b} = 1 &\mathbf{w\cdot x} + b > 0
\end{array}
\right.
$$

### 拓展: 广播函数

当使使用广播函数(board casting functions) 时，此时 $max$ 输入的参数为向量。只需要对向量中的每一个元素做标量的 $max$ 运算。即:
$$
max(0,\mathbf{x}) =
\begin{bmatrix}
max(0, x_1)\\
max(0, x_2)\\
\vdots\\
max(0, x_n)
\end{bmatrix}
$$
此时梯度为:
$$
\frac{\partial}{\partial \mathbf{x}} max(0, \mathbf{x}) = 
\begin{bmatrix}
\frac{\partial}{\partial x_1}max(0, x_1)\\
\frac{\partial}{\partial x_2}max(0, x_2)\\
\vdots\\
\frac{\partial}{\partial x_n}max(0, x_n)
\end{bmatrix}
$$

## 神经网络损失函数的梯度

损失函数的结果是一个标量，先进行符号定义 每个样本和标签被定义为 $(\mathbf{x}_i, target(\mathbf{x}_i))$  有:
$$
X=[\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N]^T 
$$
其中 $N=|X|$, 标签构成的向量为：
$$
\mathbf{y} = [target(\mathbf{x}_1), target(\mathbf{x}_2), \cdots, target(\mathbf{x}_N)]^T
$$


其中 $y_i$ 是标量。损失函数被定义为：
$$
C(\mathbf{w}, b, X,\mathbf{y})= \frac{1}{N}\sum_{i = 1}^N(y_i - activation(\mathbf{x}_i))^2=
\frac{1}{N}\sum_{i = 1}^N(y_i  - max(\mathbf{w\cdot x}_i + b))^2
$$
根据链式法则:
$$
\begin{aligned}
u(\mathbf{w}, b, \mathbf{x}) &= max(0, \mathbf{w\cdot x} + b)\\
v(y, u) &= y - u\\
C(v) &= \frac{1}{N}\sum_{i = 1}^N v^2
\end{aligned}
$$

### 关于权重的梯度



从前几章节可以知道:
$$
\frac{\partial}{\partial \mathbf{w}}u(\mathbf{w}, b, \mathbf{x}) = \left\{
\begin{array}{**lr**}
\vec{0}^T & \mathbf{w\cdot x} + b\le0\\
\mathbf{x}^T & \mathbf{w\cdot x} + b> 0
\end{array}
\right.
$$

$$
\frac{\partial v(y, u)}{\partial \mathbf{w}} = \frac{\partial}{\partial\mathbf{w}}(y - u) =
\vec{0}^T - \frac{\partial u}{\partial \mathbf{w}}=
-\frac{\partial u}{\partial\mathbf{w}} = 
\left\{
\begin{array}{**lr**}
\vec{0}^T & \mathbf{w\cdot x} + b \le 0\\
-\mathbf{x}^T &\mathbf{w\cdot x} + b > 0
\end{array}
\right.
$$
那总的梯度可以通过下式子计算：
$$
\begin{aligned}
\frac{\partial C(v)}{\partial \mathbf{w}} &=
\frac{\partial}{\partial \mathbf{w}}\frac{1}{N}
\sum_{i= 1}^N v^2\\ &=
\frac{1}{N}\sum_{i=1}^N\frac{\partial v^2}{\partial \mathbf{w}} \\&=
\frac{1}{N}\sum_{i= 1}^N 2v\frac{\partial v}{\partial \mathbf{w}}\\ &=
\frac{1}{N}\sum_{i = 1}^N\left\{
\begin{array}{lr}
2v\vec{0}^T = \vec{0}^T &\mathbf{w\cdot x}_i + b\le0\\
-2v\mathbf{x}^T & \mathbf{w\cdot x}_i + b > 0
\end{array}
\right.\\ &=
\frac{1}{N}\sum_{i=1}^N\left\{
\begin{array}{ll}
\vec{0}^T & \mathbf{w\cdot x}_i + b\le 0\\
-2(y_i - u)\mathbf{x}_i^T &\mathbf{w\cdot x}+i + b >0
\end{array}
\right.\\ &=
\frac{1}{N}\sum_{i=1}^N\left\{
\begin{array}{ll}
\vec{0}^T &\mathbf{w\cdot x}_i + b\le 0\\
-2(y_i - max(0,\mathbf{w\cdot x}+i + b))\mathbf{x}_i^T & \mathbf{w\cdot x}_i + b> 0
\end{array}
\right.\\ &=
\left\{
\begin{array}{ll}
\vec{0}^T & \mathbf{w\cdot x}_i + b \le 0\\
\frac{-2}{N}\sum_{i= 1}^N(y_i - (\mathbf{w\cdot x}_i + b))\mathbf{x}_i^T &\mathbf{w\cdot x}_i + b > 0
\end{array}
\right.\\ &=
\left\{
\begin{array}{ll}
\vec{0}^T & \mathbf{w\cdot x}_i + b\le 0\\
\frac{2}{N}\sum_{i = 1}^N(\mathbf{w\cdot x}_i + b - y_i)\mathbf{x}_i^T & \mathbf{w\cdot x}_i + b > 0
\end{array}
\right.
\end{aligned}
$$


可以定义一个误差项$e_i = \mathbf{w\cdot x}_i + b - y_i$ 来简化总梯度。注意该梯度针对激活函数结果非0的情况：
$$
\frac{\partial C}{\partial\mathbf{w}}=\frac{2}{N}\sum_{i = 1}^Ne_i\mathbf{x_i}^T \quad \mathbf{w\cdot x}_i + b > 0
$$
注意此时梯度是通过所有样本计算的加权平均项。权重与误差项相关。最终的梯度指向更大的 $e_i$ 对应样本的方向。梯度下降公式写作 
$$
\mathbf{w}_{t + 1} = \mathbf{w}_t - \mathbf{\eta}\frac{\partial C}{\partial\mathbf{w}} 
$$


### 针对偏置项的公式

优化偏置项 $b$ 和优化权重项类似， 先使用中间变量
$$
\begin{aligned}
u(\mathbf{w}, b, \mathbf{x}) &= max(0, \mathbf{w\cdot x} + b)\\
v(y, u) &= y - i\\
C(v) &= \frac{1}{N}\sum_{i=1}^N v^2
\end{aligned}
$$
已知：
$$
\frac{\partial u}{\partial b} = \left\{
\begin{array}{ll}
0 & \mathbf{w\cdot x} + b \le 0\\
1 & \mathbf{w\cdot x} + b> 0
\end{array}
\right.
$$
那么对于 $v$ , 其偏导数为:
$$
\frac{\partial v(y, u)}{\partial b} = -\frac{\partial u}{\partial b} = \left\{
\begin{array}{ll}
0  & \mathbf{w\cdot x} + b \le 0\\
-1 & \mathbf{w\cdot x} + b > 0
\end{array}
\right.
$$
那么总的偏导数为
$$
\begin{aligned}
\frac{\partial C(v)}{\partial b} &=
\frac{\partial}{\partial b}\frac{1}{N}
\sum_{i = 1}^Nv ^ 2\\ &=
\frac{1}{N}\sum_{i = 1}^N
\frac{\partial}{\partial b} v^2\\ &=
\frac{1}{N}\sum_{i = 1}^N
2v\frac{\partial v}{\partial b}\\ &=
\frac{1}{N}\sum_{i = 1}^N\left\{
\begin{array}{ll}
0 & \mathbf{w\cdot x} + b\le 0\\
-2v & \mathbf{w\cdot x} + b > 0
\end{array}
\right.\\ &=
\frac{1}{N}\sum_{i = 1}^N\left\{
\begin{array}{ll}
0 & \mathbf{w\cdot x} + b \le 0\\
-2(y_i - max(0, \mathbf{w\cdot x}_i + b) &\mathbf{w \cdot} x + b > 0
\end{array}
\right. \\ &=
\left\{
\begin{array}{ll}
0 & \mathbf{w\cdot x} + b \le 0\\
\frac{2}{N}\sum_{i = 1}^N2(\mathbf{w\cdot x}_i + b - y_i) &\mathbf{w \cdot x}_i + b > 0
\end{array}
\right.

\end{aligned}
$$
同理通过定于误差项
$$
e_i = \mathbf{w\cdot x}_i + b - y_i
$$


 可以得到偏导数为
$$
\frac{\partial C}{\partial b} = \frac{2}{N}\sum_{i = 1}^Ne_i \quad \mathbf{w\cdot x}_i + b > 0
$$
对应的更新公式为
$$
b_{t + 1} = b_t - \mathbf{\eta}\frac{\partial C}{\partial b}
$$


### 总结

在实际使用过程中，通常使用扩充权重向量。即
$$
\begin{aligned}
\hat{\mathbf{w}} &= [\mathbf{w}^T, b]^T\\
\hat{\mathbf{x}} & = [\mathbf{x}^T, 1] 
\end{aligned}
$$
 此时 $\mathbf{w\cdot x} + b = \hat{\mathbf{w}}\cdot \hat{\mathbf{x}}$

##  参考文献

[Wolfram Alpha](http://www.wolframalpha.com/input/?i=D[{x^2,+x^3}.{{1,2},{3,4}}.{x^2,+x^3},+x]) can do symbolic matrix algebra and there is also a cool dedicated [matrix calculus differentiator](http://www.matrixcalculus.org/).

When looking for resources on the web, search for “matrix calculus” not “vector calculus.” Here are some comments on the top links that come up from a [Google search](https://www.google.com/search?q=matrix+calculus&oq=matrix+calculus):

- https://en.wikipedia.org/wiki/Matrix_calculus

  The Wikipedia entry is actually quite good and they have a good description of the different layout conventions. Recall that we use the numerator layout where the variables go horizontally and the functions go vertically in the Jacobian. Wikipedia also has a good description of [total derivatives](https://en.wikipedia.org/wiki/Total_derivative), but be careful that they use slightly different notation than we do. We always use the ![img](https://explained.ai/matrix-calculus/images/eqn-28DDB3E82115D069796FAF6356E2DBF6-depth000.27.svg) notation not *dx*.

- http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/calculus.html

  This page has a section on matrix differentiation with some useful identities; this person uses numerator layout. This might be a good place to start after reading this article to learn about matrix versus vector differentiation.

- https://www.colorado.edu/engineering/CAS/courses.d/IFEM.d/IFEM.AppC.d/IFEM.AppC.pdf

  This is part of the course notes for “Introduction to Finite Element Methods” I believe by [Carlos A. Felippa](https://www.colorado.edu/engineering/CAS/courses.d/IFEM.d). His Jacobians are transposed from our notation because he uses denominator layout.

- http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/calculus.html

  This page has a huge number of useful derivatives computed for a variety of vectors and matrices. A great cheat sheet. There is no discussion to speak of, just a set of rules.

- https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

  Another cheat sheet that focuses on matrix operations in general with more discussion than the previous item.

- https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation.pdf

  A useful set of slides.

To learn more about neural networks and the mathematics behind optimization and back propagation, we highly recommend [Michael Nielsen's book](http://neuralnetworksanddeeplearning.com/chap1.html).

For those interested specifically in convolutional neural networks, check out [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf).

We reference the law of [total derivative](https://en.wikipedia.org/wiki/Total_derivative), which is an important concept that just means derivatives with respect to *x* must take into consideration the derivative with respect *x* of all variables that are a function of *x*.